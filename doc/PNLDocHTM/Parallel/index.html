<html>
<title>Parallel Algorithms</title>
<link rel="stylesheet" type="text/css" href="css/table.css">
<body>
<table>
<tr><td width="100%" bgcolor="#0000ff"><h2><font color="ffffff">Parallel Algorithms</font></h2></td></tr>
<tr><td>
	<p><font class="method">Junction Tree Inference Engine</font></p>
	<p>Junction Tree algorithm is constructed to carry out inference in Bayesian and Markov nets. On the base of initial net, the cliques tree (Junction Tree) is constructed. And then two steps inference is carried out. The first step is information gathering from leaves to the root. The second step is information distribution from root to the leaves. <br>
	<pre>			<a href="ParJtree_example.html ">Example of usage</a></pre></p>

	<p><font class="method">Pearl Inference Engine</font></p>
	<p>Loopy Belief Propagation is designed to carry out inference in Bayesian and Markov nets. The idea of this algorythm is calculation of the component of belifs vector for every node. For networks with adirected cycles this algorithm gives the proximate result.</p>
	<pre>			<a href="ParPearl_example.html ">Example of usage</a></pre></p>

	<p><font class="method">Gibbs Sampling</font></p>
	<p>The goal of this algorithm is to get the marginal distribution on definite nodes on the base of some evidences. Distribution calculation is carried out by generating random samples.</p>
	<pre>			<a href="ParGibbs_example.html ">Example of usage</a></pre></p>

	<p><font class="method">Expectation Maximization Learning</font></p>
	<p>This algorithm is designed to carry out inference in Bayesian and Markov nets on the base of learning with definite samples. We propose that there are observable and hidden nodes in the net. Distributions on the nodes are unknown, but belong to some class. On the base of evidences on observable nodes, the probability distribution on hidden nodes  is calculated.</p>
	<pre>			<a href="ParEMLearn_example.html ">Example of usage</a></pre></p>
</td></tr>
</table>
</body>
</html>